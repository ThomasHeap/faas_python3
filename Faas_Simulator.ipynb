{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Faas Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to import basic packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATH\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sbi\n",
    "import sbi\n",
    "import sbi.utils as utils\n",
    "from sbi.inference.snpe.snpe_c import SnpeC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove top and right axis from plots\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different required components\n",
    "\n",
    "Before running inference, let us define the different required components:\n",
    "\n",
    "1. observed data\n",
    "2. simulator\n",
    "4. prior over model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faas_helper_fns import simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the input current and the simulator together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_faas_model(params):\n",
    "    \n",
    "    #params = np.asarray(params)\n",
    "    \n",
    "    states = simulator(th=params)\n",
    "    t = np.genfromtxt('data/time_points.csv', delimiter=',')\n",
    "    \n",
    "    return {'data': states,\n",
    "            'time': t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three sets of (g_Na, g_K)\n",
    "theta_1 = [8.886491e+00,  7.924279e+00,\n",
    "   1.050515e+01,  7.397940e+00, -3.682371e+00, -4.509306e+00, -6.162727e+00,\n",
    "  -6.585027e+00,  1.100000e-03, -3.900000e-01] \n",
    "\n",
    "theta_2 = [8.886491e+00,  7.924279e+00,\n",
    "   1.050515e+01,  7.397940e+00, -3.682371e+00, -4.509306e+00, -6.162727e+00,\n",
    "  -6.585027e+00,  1.100000e-03, -3.900000e-01] \n",
    "\n",
    "\n",
    "params = np.array([theta_1,theta_2])\n",
    "\n",
    "t = np.genfromtxt('data/time_points.csv', delimiter=',')\n",
    "                  \n",
    "num_samples = len(params[:,0])\n",
    "sim_samples = np.zeros((num_samples, 94 ,259))\n",
    "\n",
    "\n",
    "sim_samples[:num_samples,:,:] = run_faas_model(params)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# colors for traces\n",
    "col_min = 2\n",
    "num_colors = num_samples+col_min\n",
    "cm1 = mpl.cm.Blues\n",
    "col1 = [cm1(1.*i/num_colors) for i in range(col_min,num_colors)]\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in np.arange(92//4):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    for i in range(num_samples):\n",
    "        ax1.plot(t,sim_samples[i,j*4,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax1.set_title(str(j*4+1))\n",
    "        ax2.plot(t,sim_samples[i,j*4+1,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax2.set_title(str(j*4+2))\n",
    "        ax3.plot(t,sim_samples[i,j*4+2,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax3.set_title(str(j*4+3))\n",
    "        ax4.plot(t,sim_samples[i,j*4+3,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax4.set_title(str(j*4+4))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plt.figure(figsize=(20,10))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)        \n",
    "for i in range(num_samples):\n",
    "    ax1.plot(t,sim_samples[i,92,:],color=col1[i],lw=2, label = str(i))\n",
    "    ax1.set_title(str(93))\n",
    "    ax2.plot(t,sim_samples[i,93,:],color=col1[i],lw=2, label = str(i))\n",
    "    ax2.set_title(str(94))\n",
    "plt.ylabel('voltage (mV)')\n",
    "plt.legend(loc=\"upper right\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "#fig.set_xticks([])\n",
    "#fig.set_yticks([-80, -20, 40])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the voltage traces can be quite diverse for different parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we are not interested in matching the exact trace, but only in matching certain features thereof. In this example of the Hodgkin Huxley model, the summary features are the number of spikes, the mean resting potential, the standard deviation of the resting potential, and the first 4 voltage moments, mean, standard deviation, skewness and kurtosis. In the function `calculate_summary_statistics()` below, we compute these statistics from the output of the Hodgkin Huxley simulator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faas_helper_fns import calc_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define a function that performs all of the above steps at once. The function `simulation_wrapper` takes in conductance values, runs the Hodgkin Huxley model and then returns the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_wrapper(params):\n",
    "    \"\"\"\n",
    "    Takes in conductance values and then first runs the Hodgkin Huxley model and then returns the summary statistics as torch.Tensor\n",
    "    \"\"\"\n",
    "    obs = run_faas_model(params)\n",
    "    summstats = torch.as_tensor(calc_summ(d=obs))\n",
    "    return summstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sbi` takes any function as simulator. Thus, `sbi` also has the flexibility to use simulators that utilize external packages, e.g., Brian (http://briansimulator.org/), nest (https://www.nest-simulator.org/), or NEURON (https://neuron.yale.edu/neuron/). External simulators do not even need to be Python-based as long as they store simulation outputs in a format that can be read from Python. All that is necessary is to wrap your external simulator of choice into a python callable that takes a parameter set and outputs a set of summary statistics we want to fit the parameters to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for traces\n",
    "col_min = 2\n",
    "num_colors = num_samples+col_min\n",
    "cm1 = mpl.cm.Blues\n",
    "col1 = [cm1(1.*i/num_colors) for i in range(col_min,num_colors)]\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in np.arange(1):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    for i in range(num_samples):\n",
    "        ax1.plot(t,sim_samples[i,j*4,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax1.set_title(str(j*4+1))\n",
    "        ax2.plot(t,sim_samples[i,j*4+1,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax2.set_title(str(j*4+2))\n",
    "        ax3.plot(t,sim_samples[i,j*4+2,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax3.set_title(str(j*4+3))\n",
    "        ax4.plot(t,sim_samples[i,j*4+3,:],color=col1[i],lw=2, label = str(i))\n",
    "        ax4.set_title(str(j*4+4))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "for i in range(num_samples):\n",
    "    print(simulation_wrapper(params=params[i,:])[:5])\n",
    "\n",
    "print(sim_samples[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prior over model parameters\n",
    "\n",
    "Now that we have the simulator, we need to define a function with the prior over the model parameters ($\\bar g_{Na}$,$\\bar g_K$), which in this case is chosen to be a Uniform distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_max = np.array([12,  10, 12,  10, -3, -4, -4,-4] + [0.05,0.5])\n",
    "prior_min = np.array([4,  2, 4, 2, -10, -10, -10, -12] + [-0.05,-0.5])\n",
    "prior = utils.torchutils.BoxUniform(low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming back to the observed data\n",
    "As mentioned at the beginning of the tutorial, the observed data are generated by the Hodgkin-Huxley model with a set of known parameters ($\\bar g_{Na}$,$\\bar g_K$). To illustrate how to compute the summary statistics of the observed data, let us regenerate the observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true parameters and respective labels\n",
    "true_params = np.array([8.886491e+00,  7.924279e+00,\n",
    "   1.050515e+01,  7.397940e+00, -3.682371e+00, -4.509306e+00, -6.162727e+00,\n",
    "  -6.585027e+00,  1.100000e-03, -3.900000e-01])       \n",
    "labels_params = ['KonTN', 'KonTC', 'KonRN', 'KonRC', 'KDTN', 'KDTC', 'KDRN', 'KDRC', 'malpha', 'alpha0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "observation_trace = run_faas_model(torch.FloatTensor([true_params]))\n",
    "observation_summary_statistics = torch.as_tensor(calc_summ(observation_trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had already shown above, the observed voltage traces look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "t = observation_trace['time']\n",
    "num_samples = 1\n",
    "for j in np.arange(92//4):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    ax1.plot(t,observation_trace['data'][j*4,:],lw=2)\n",
    "    ax1.set_title(str(j*4+1))\n",
    "    ax2.plot(t,observation_trace['data'][j*4+1,:],lw=2)\n",
    "    ax2.set_title(str(j*4+2))\n",
    "    ax3.plot(t,observation_trace['data'][j*4+2,:],lw=2)\n",
    "    ax3.set_title(str(j*4+3))\n",
    "    ax4.plot(t,observation_trace['data'][j*4+3,:],lw=2)\n",
    "    ax4.set_title(str(j*4+4))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plt.figure(figsize=(20,10))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)        \n",
    "\n",
    "ax1.plot(t,observation_trace['data'][92,:],lw=2)\n",
    "ax1.set_title(str(93))\n",
    "ax2.plot(t,observation_trace['data'][93,:],lw=2)\n",
    "ax2.set_title(str(94))\n",
    "plt.ylabel('voltage (mV)')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that we have all the required components, we can run inference with SNPE. We start by importing our SNPE object of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use SNPE to identify parameters whose activity matches this trace. To do so, we instantiate the SNPE object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snpe_common_args = dict(\n",
    "    simulator=simulation_wrapper,\n",
    "    x_o=observation_summary_statistics,\n",
    "    prior=prior,\n",
    "    simulation_batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and run the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ce451e46644c1b4a7a862c4c0b412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a5713c355542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Run inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_simulations_per_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m posterior = infer(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_simulations_per_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_simulations_per_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/Thesis/sbi/sbi/inference/snpe/snpe_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, num_rounds, num_simulations_per_round, batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Run simulations for the round.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_simulations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;31m# XXX Rename bank -> rounds/roundwise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_theta_bank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/sbi/sbi/inference/snpe/snpe_base.py\u001b[0m in \u001b[0;36m_run_simulations\u001b[0;34m(self, round_, num_sims)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mround_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batched_simulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;31m# What is happening here? By design, we want the neural net to take care of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# normalizing both input and output, x and theta. But since we don't know\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/sbi/sbi/inference/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_round_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_round_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         self._batched_simulator = lambda theta: simulate_in_batches(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulation_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_progressbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         )\n",
      "\u001b[0;32m~/Thesis/sbi/sbi/simulators/simutils.py\u001b[0m in \u001b[0;36msimulate_in_batches\u001b[0;34m(simulator, theta, sim_batch_size, show_progressbar)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0msimulation_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0msimulation_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/sbi/sbi/user_input/user_input_checks.py\u001b[0m in \u001b[0;36mpytorch_simulator\u001b[0;34m(theta)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Define a wrapper to make sure that the output of the simulator is float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpytorch_simulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpytorch_simulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-98c4025267c0>\u001b[0m in \u001b[0;36msimulation_wrapper\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTakes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconductance\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0mruns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mHodgkin\u001b[0m \u001b[0mHuxley\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_faas_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msummstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_summ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-351cf4599a3a>\u001b[0m in \u001b[0;36mrun_faas_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#params = np.asarray(params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/time_points.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Thesis/sbi/examples/faas_python3/faas_helper_fns.py\u001b[0m in \u001b[0;36msimulator\u001b[0;34m(th)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"forkserver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sbi/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer = SnpeC(sample_with_mcmc=False, **snpe_common_args)\n",
    "\n",
    "# Run inference.\n",
    "num_rounds, num_simulations_per_round = 1, 100\n",
    "posterior = infer(\n",
    "    num_rounds=num_rounds, num_simulations_per_round=num_simulations_per_round, batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results\n",
    "\n",
    "After running the inference algorithm, let us inspect and analyse the results. It seems that the loss function has converged. Let us inspect the inferred posterior distribution over the parameters ($\\bar g_{Na}$,$\\bar g_K$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# colors\n",
    "hex2rgb = lambda h: tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "# RGB colors in [0, 255]\n",
    "col = {}\n",
    "col['GT']      = hex2rgb('30C05D')\n",
    "col['SNPE']    = hex2rgb('2E7FE8')\n",
    "col['SAMPLE1'] = hex2rgb('8D62BC')\n",
    "col['SAMPLE2'] = hex2rgb('AF99EF')\n",
    "\n",
    "# convert to RGB colors in [0, 1]\n",
    "for k, v in col.items():\n",
    "    col[k] = tuple([i/255 for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = utils.samples_nd(utils.tensor2numpy(samples),\n",
    "                       limits=np.asarray([prior_min, prior_max]).T,\n",
    "                       ticks=np.asarray([prior_min, prior_max]).T,\n",
    "                       fig_size=(5,5),\n",
    "                       diag='kde',\n",
    "                       upper='kde',\n",
    "                       hist_diag={'bins': 50},\n",
    "                       hist_offdiag={'bins': 50},\n",
    "                       kde_diag={'bins': 50, 'color': col['GT']},\n",
    "                       kde_offdiag={'bins': 50},\n",
    "                       points=[true_params],\n",
    "                       points_offdiag={'markersize': 5},\n",
    "                       points_colors='g',\n",
    "                       title='');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the inferred posterior contains the ground-truth parameters (green) in a high-probability region. Now, let us sample parameters from the posterior distribution, simulate the Hodgkin-Huxley model for each of those samples and compare the simulations with the observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = observation_trace['data']\n",
    "t = observation_trace['time']\n",
    "duration = np.max(t)\n",
    "\n",
    "num_samp = 2\n",
    "\n",
    "# sample from posterior\n",
    "x_samp = posterior.sample(num_samples=1000)\n",
    "\n",
    "x_samp = np.median(x_samp.detach().numpy(), axis=0)\n",
    "\n",
    "#x_samp = [ 7.682584,    5.845819,    8.078228,    6.199831,   -6.098556,   -6.454344,\n",
    "# -6.7167273,  -8.592197,   -0.01400174, -0.0039]\n",
    "\n",
    "#[8.886491e+00,  7.924279e+00,\n",
    "#1.050515e+01,  7.397940e+00, -3.682371e+00, -4.509306e+00, -6.162727e+00,\n",
    "#-6.585027e+00,  1.100000e-03, -3.900000e-01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "col_min = 1\n",
    "num_colors = num_samp+col_min\n",
    "cm1 = mpl.cm.Blues\n",
    "col1 = [cm1(1.*i/num_colors) for i in range(col_min,num_colors)]\n",
    "\n",
    "# reject samples for which prior is zero\n",
    "#ind = (x_samp > prior_min) & (x_samp < prior_max)\n",
    "#params = x_samp[np.prod(ind,axis=1)==1]\n",
    "params = x_samp\n",
    "#num_samp = len(params[:,0])\n",
    "\n",
    "# simulate and plot samples\n",
    "post_samples = np.zeros((num_samp, 94 ,259))\n",
    "post_samples = run_faas_model(params=params)['data']\n",
    "\n",
    "    \n",
    "for j in np.arange(92//4):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    ax1.plot(t,observation_trace['data'][j*4,:],lw=2)\n",
    "    ax1.set_title(str(j*4+1))\n",
    "    ax2.plot(t,observation_trace['data'][j*4+1,:],lw=2)\n",
    "    ax2.set_title(str(j*4+2))\n",
    "    ax3.plot(t,observation_trace['data'][j*4+2,:],lw=2)\n",
    "    ax3.set_title(str(j*4+3))\n",
    "    ax4.plot(t,observation_trace['data'][j*4+3,:],lw=2)\n",
    "    ax4.set_title(str(j*4+4))\n",
    "\n",
    "    #for i in range(num_samp):\n",
    "    ax1.plot(t,post_samples[j*4,:],color=col1[0],lw=2, label = str(0))\n",
    "    ax1.set_title(str(j*4+1))\n",
    "    ax2.plot(t,post_samples[j*4+1,:],color=col1[0],lw=2, label = str(0))\n",
    "    ax2.set_title(str(j*4+2))\n",
    "    ax3.plot(t,post_samples[j*4+2,:],color=col1[0],lw=2, label = str(0))\n",
    "    ax3.set_title(str(j*4+3))\n",
    "    ax4.plot(t,post_samples[j*4+3,:],color=col1[0],lw=2, label = str(0))\n",
    "    ax4.set_title(str(j*4+4))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plt.figure(figsize=(20,10))\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)        \n",
    "\n",
    "ax1.plot(t,observation_trace['data'][92,:],lw=2)\n",
    "ax1.set_title(str(93))\n",
    "ax2.plot(t,observation_trace['data'][93,:],lw=2)\n",
    "ax2.set_title(str(94))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the samples from the inferred posterior lead to simulations that closely resemble the observed data, confirming that `SNPE-C` did a good job at capturing the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500–544, 1952.\n",
    "\n",
    "M. Pospischil, M. Toledo-Rodriguez, C. Monier, Z. Piwkowska, T. Bal, Y. Frégnac, H. Markram, and A. Destexhe. Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons. Biological Cybernetics, 99(4-5), 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
